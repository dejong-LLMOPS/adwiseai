{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11bf539d-ef2b-474f-90c9-111b51a74764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AdWiseAI â€“ Data Ingestion (Sprint 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bbb83e-411a-4673-b5ef-818495814ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Azure ADLS2 Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3a6167-abde-4bbe-8a91-e9971b5da0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ADLS Configuration\n",
    "ACCOUNT_NAME = \"llmops\"\n",
    "FILE_SYSTEM_NAME = \"raw\"\n",
    "ADS_FILE_PATH = \"ads_dataset.csv\"\n",
    "LOCAL_TEMP_PATH = \"/dbfs/tmp/ads_dataset.csv\"  # Use /dbfs so Spark can access it\n",
    "\n",
    "# Authenticate with Managed Identity (Credential Passthrough)\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Get the file system client\n",
    "file_system_client = service_client.get_file_system_client(FILE_SYSTEM_NAME)\n",
    "\n",
    "# Optional: List files to verify the dataset path\n",
    "print(\"Listing files in container:\")\n",
    "for path in file_system_client.get_paths():\n",
    "    print(\" -\", path.name)\n",
    "\n",
    "# Download the CSV file from ADLS and save to DBFS\n",
    "try:\n",
    "    ads_file_client = file_system_client.get_file_client(ADS_FILE_PATH)\n",
    "    downloaded = ads_file_client.download_file()\n",
    "    ads_csv_data = downloaded.readall()\n",
    "\n",
    "    with open(LOCAL_TEMP_PATH, \"wb\") as f:\n",
    "        f.write(ads_csv_data)\n",
    "\n",
    "    # Load CSV into Spark DataFrame\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/tmp/ads_dataset.csv\")\n",
    "    display(df)\n",
    "    df.printSchema()\n",
    "\n",
    "    # Set current database (create manually beforehand if needed)\n",
    "    spark.catalog.setCurrentDatabase(\"adwiseai\")\n",
    "\n",
    "    # Save as Delta Table\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS adwiseai\") ### USE WHEN INIITIALIZING DB\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"adwiseai.ads_dataset_raw\")\n",
    "    print(\"Ingestion complete. Delta table saved: adwiseai.ads_dataset_raw\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error downloading or reading file:\")\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d517639b-98d9-400b-a4e5-9f87e2a970ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Local Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b43f9b-5f69-4150-988b-828534cd5653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Path to the workspace CSV file\n",
    "# # Copy file from workspace path to DBFS\n",
    "# source_path = \"file:/Workspace/Users/njdejong99@gmail.com/adwiseai/data/ads_dataset.csv\"\n",
    "# target_path = \"dbfs:/tmp/ads_dataset.csv\"\n",
    "\n",
    "# dbutils.fs.cp(source_path, target_path)\n",
    "\n",
    "# file_path = \"file:/Workspace/Users/njdejong99@gmail.com/adwiseai/data/ads_dataset.csv\"\n",
    "\n",
    "# # Read into Spark DataFrame\n",
    "# df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# # Show schema\n",
    "# df.printSchema()\n",
    "\n",
    "\n",
    "# #spark.sql(\"CREATE DATABASE IF NOT EXISTS adwiseai\") ## USE WHEN INIITIALIZING DB\n",
    "\n",
    "# spark.catalog.setCurrentDatabase(\"adwiseai\")\n",
    "\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"adwiseai.ads_dataset_raw\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
